{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "import multiprocessing\n",
    "from tqdm import tqdm, trange\n",
    "import operator\n",
    "import pickle\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_occ = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#files\n",
    "trainDir = \"aclimdb/train/\"\n",
    "testDir = \"aclimdb/test/\"\n",
    "contenttrain = []\n",
    "labeltrain = []\n",
    "contenttest = []\n",
    "labeltest = []\n",
    "#extracting data\n",
    "for i in listdir(trainDir+'pos'):\n",
    "    contenttrain.append(open(trainDir+'pos/'+i,'r').read())\n",
    "    labeltrain.append('1')\n",
    "for i in listdir(trainDir+'neg'):\n",
    "    contenttrain.append(open(trainDir+'neg/'+i,'r').read())\n",
    "    labeltrain.append('0')\n",
    "for i in listdir(testDir+'pos'):\n",
    "    contenttest.append(open(testDir+'pos/'+i,'r').read())\n",
    "    labeltest.append('1')\n",
    "for i in listdir(testDir+'neg'):\n",
    "    contenttest.append(open(testDir+'neg/'+i,'r').read())\n",
    "    labeltest.append('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vocab_sentences(sents):\n",
    "    lexicon = []\n",
    "    sentences = []\n",
    "    for sent in tqdm(sents):\n",
    "        \n",
    "        refSent = ''.join(('' if i.isdigit() else i) for i in (sent) if (i not in string.punctuation) ).lower()\n",
    "        allWords = word_tokenize(refSent)\n",
    "        sentences.append(allWords)\n",
    "        lexicon += list(allWords)\n",
    "        \n",
    "#     lexicon = [lemmatizer.lemmatize(i) for i in tqdm(lexicon)]\n",
    "    w_counts = Counter(lexicon)\n",
    "    lexicon2 =[]\n",
    "    for w in w_counts:\n",
    "        if w_counts[w]>min_occ:\n",
    "            lexicon2.append(w)\n",
    "    lexicon2.append('unk')\n",
    "    lexicon2 = set(lexicon2)\n",
    "    sentences = [[w if w in lexicon2 else 'unk' for w in s] for s in tqdm(sentences)]\n",
    "    return sentences,lexicon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word to vec\n",
    "def w2v(sentences):\n",
    "    cores = multiprocessing.cpu_count()\n",
    "    global model\n",
    "    model = Word2Vec(sente, min_count=1, size=200, sg=1, iter=2, negative=10, workers=cores)\n",
    "    model.save('./model/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/25000 [00:00<00:54, 460.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:36<00:00, 676.50it/s]\n",
      "100%|██████████| 25000/25000 [00:00<00:00, 26345.16it/s]\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#sentences and vocabulary\n",
    "sente,vocab = vocab_sentences(contenttrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 2 µs, total: 7 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#word to vec\n",
    "w2v(sente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# index to word dictionary\n",
    "id_to_word = dict(enumerate(list(vocab)))\n",
    "# word to index dictionary\n",
    "word_to_id = {v: k for k, v in id_to_word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentence vector\n",
    "sentVector = [[word_to_id[word] for word in line] for line in sente]\n",
    "#maximum length vector\n",
    "max_length = max([len(sent) for sent in sentVector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:00<00:00, 30523.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# pad 0's to the end of each sequence\n",
    "for i in trange(len(sentVector)):\n",
    "    sentVector[i] = sentVector[i] + [0]*(max_length - len(sentVector[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initalize an embeddings array which contains word vectors for all the words in vocabulary. These word vectors can \n",
    "# be accesed by the indices from this vector\n",
    "embed = np.zeros((len(vocab)+1, 200))\n",
    "for k,v in word_to_id.items():\n",
    "    embed[v] = model[k]\n",
    "# assigning 0's vector of size 200 for padding\n",
    "# note: 0 index in vocabulary was assigned to empty word ''\n",
    "embed[0] = np.zeros((200))\n",
    "trainX = np.array(sentVector)\n",
    "trainY = np.array(labeltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savez('model/train_set.npz', trainX=trainX, trainY=trainY)\n",
    "np.savez('model/embedding.npz', embed=embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
